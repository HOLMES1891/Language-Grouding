# Language Grouding
***
Nowadays Deep Learning models (such as Bert) have largely promoted the development of Natural Language Processing. The main idea is to learn language model from large corpus. Despite the great success on many different tasks, such as QA, deep learning models still fail to really **understand** what human language is talking about.<br>
Here is a reading list for Language Grounding and some related topics. Although these works have not draw much attention yet, I believe they are important and indispensable for AGI.
***
1. [**Language Grounding to Vision and Control**](https://katefvision.github.io/LanguageGrounding/)
2. [**Modularity of Mind**](https://plato.stanford.edu/entries/modularity-mind/#WhatMentModu) & [**Is the mind really modular?**](http://www.subcortex.com/PrinzModularity.pdf)
3. [**CraftAssist: A Framework for Dialogue-enabled Interactive Agents**](https://arxiv.org/abs/1907.08584)
4. [**A Computational Theory of Grounding in Natural Language Conversation**](https://apps.dtic.mil/dtic/tr/fulltext/u2/a289894.pdf)

***
Some active researchers in this field:

* [Igor Labutov](https://igorlabutov.com) (LAER AI) 
* [Bishan Yang](http://www.cs.cmu.edu/~bishan/) (LAER AI)

