# Language and The World
<br>

Nowadays Deep Learning models (such as Bert) have largely promoted the development of Natural Language Processing. The main idea is to learn language model from large corpus. Despite the great success on many different NLP tasks, such as QA, deep learning models still fail to really **understand** what human language. The thought experiment ["Chinese Room"](https://en.wikipedia.org/wiki/Chinese_room) tells why.
<br>

In my opinion, people learn language by understanding the world. In human mind, we have a model for language, and also a model for the physical world. Just as the introduction of Knowledge Graph from Google:

>"things not strings".

Therefore, we must let our deep learning models to learn lanaguage from multi-dimensional data: video, audio, etc. **Grounding** symbols into real world objects is a prerequisite for natural language understanding (NLU), which is a small step towards human-level intelligence.


## Papers:
### Language Grounding in Game
* [**Why Build an Assistant in Minecraft?**](https://research.fb.com/publications/why-build-an-assistant-in-minecraft/)
* [**BabyAI: First Steps Towards Grounding Language Learning With A Human In The Loop**](https://arxiv.org/pdf/1810.08272.pdf)
* [**A Computational Theory of Grounding in Natural Language Conversation**](https://apps.dtic.mil/dtic/tr/fulltext/u2/a289894.pdf)

### Visual Question Answering / Multimodal ML
* [**Grounded Semantic Role Labeling**](https://www.aclweb.org/anthology/N16-1019.pdf)
* [**Neural-Symbolic VQA: Disentangling Reasoning from Vision and Language Understanding**]()
* [**TVQA+: Spatio-Temporal Grounding for Video Question Answering**](https://arxiv.org/pdf/1904.11574.pdf)

### Mind
* [**Modularity of Mind**](https://plato.stanford.edu/entries/modularity-mind/#WhatMentModu)
* [**Is the mind really modular?**](http://www.subcortex.com/PrinzModularity.pdf)

### Compositional and disentangled representations
* [**Reconciling deep learning with symbolic artiÔ¨Åcial intelligence: representing objects and relations**](https://www.sciencedirect.com/science/article/pii/S2352154618301943)

## Courses:
* [**Language Grounding to Vision and Control**](https://katefvision.github.io/LanguageGrounding/#readings)
* [**Grounded Natural Language Processing**](https://www.cs.utexas.edu/~mooney/gnlp/)
* [**Grounded Language for Robotics**](http://www.cs.unc.edu/~mbansal/teaching/robonlp-seminar-spring17.html) 

## Active Researcher:
* [Mohit Bansal](http://www.cs.unc.edu/~mbansal/prospective-students.html) (UNC)
* [Igor Labutov](https://igorlabutov.com) (LAER AI) 
* [Bishan Yang](http://www.cs.cmu.edu/~bishan/) (LAER AI)
* [Yoshua Bengio](https://mila.quebec/en/yoshua-bengio/) (MILA)
* [Joyce Y. Chai](http://www.cse.msu.edu/~jchai/) (Umich)
* [David Traum](http://people.ict.usc.edu/~traum/)(USC)
* [Raymond J. Mooney](https://www.cs.utexas.edu/users/mooney/)(UTA)
* [Yonatan Bisk](https://yonatanbisk.com)(CMU)
* [Marta Garnelo](https://www.martagarnelo.com)(DeepMind)





